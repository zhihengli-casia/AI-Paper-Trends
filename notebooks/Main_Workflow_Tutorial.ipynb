{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ•™ç¨‹ï¼šä½¿ç”¨ Notebook è¿è¡Œè‡ªåŠ¨åŒ–åˆ†ææµç¨‹\n",
    "\n",
    "æœ¬ Notebook ç”¨äºåˆ†æ­¥æ‰§è¡Œ `main.py` çš„æ ¸å¿ƒé€»è¾‘ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "project_root = current_dir.parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src import get_papers, run_topic_modeling, analyze\n",
    "\n",
    "print(f\"âœ… Environment setup complete. Project root set to: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤äºŒï¼šæŒ‡å®šé…ç½®æ–‡ä»¶\n",
    "\n",
    "åœ¨æ­¤å•å…ƒæ ¼ä¸­ï¼ŒæŒ‡å®šè¦è¿è¡Œçš„é…ç½®æ–‡ä»¶è·¯å¾„å’Œæ˜¯å¦å¼ºåˆ¶é‡è·‘ï¼Œä»¥æ¨¡æ‹Ÿå‘½ä»¤è¡Œè¾“å…¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User Configuration Area ---\n",
    "\n",
    "# 1. Specify the path to your configuration file\n",
    "config_path = \"../configs/iclr_2025_full_analysis.yaml\"\n",
    "\n",
    "# 2. (Optional) Set to True to force rerun all steps\n",
    "force_rerun = False\n",
    "\n",
    "# ------------------------------------\n",
    "\n",
    "# Simulate argparse\n",
    "class Args:\n",
    "    def __init__(self, config, force_rerun):\n",
    "        self.config = config\n",
    "        self.force_rerun = force_rerun\n",
    "\n",
    "args = Args(config_path, force_rerun)\n",
    "\n",
    "print(f\"Configuration file to be used: {args.config}\")\n",
    "print(f\"Force rerun enabled: {args.force_rerun}\")\n",
    "\n",
    "if not Path(args.config).exists():\n",
    "    print(f\"âŒ Error: Configuration file '{args.config}' not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ä¸‰ï¼šåŠ è½½é…ç½®å¹¶å‡†å¤‡ç›®å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str) -> dict:\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "def get_expected_filepaths(config: dict, raw_dir: Path, processed_dir: Path) -> dict:\n",
    "    conf_id = config['conference_id'].replace('/', '_').replace('.', '')\n",
    "    limit = config.get('limit', None)\n",
    "    fetch_reviews = config.get('fetch_reviews', False)\n",
    "\n",
    "    suffix = \"_reviews\" if fetch_reviews else \"\"\n",
    "    limit_suffix = f\"_limit{limit}\" if limit else \"\"\n",
    "    raw_filename = f\"{conf_id}_papers{suffix}{limit_suffix}.jsonl\"\n",
    "    raw_path = raw_dir / raw_filename\n",
    "\n",
    "    processed_filename = f\"{raw_path.stem}_with_topics.csv\"\n",
    "    processed_path = processed_dir / processed_filename\n",
    "    \n",
    "    return {\"raw\": raw_path, \"processed\": processed_path}\n",
    "\n",
    "print(f\"ğŸ“– Loading configuration from: {args.config}\")\n",
    "config = load_config(args.config)\n",
    "\n",
    "output_folder_name = config.get('output_folder_name', 'default_analysis')\n",
    "output_dir = Path(\"results\") / output_folder_name\n",
    "data_raw_dir = Path(\"data/raw\")\n",
    "data_processed_dir = Path(\"data/processed\")\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"ğŸ“‚ Results will be saved to: {output_dir}\")\n",
    "\n",
    "expected_paths = get_expected_filepaths(config, data_raw_dir, data_processed_dir)\n",
    "raw_data_path = expected_paths['raw']\n",
    "processed_data_path = expected_paths['processed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤å››ï¼šæ•°æ®è·å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- [Step 1/3] Data Fetching ---\")\n",
    "if not args.force_rerun and raw_data_path.exists():\n",
    "    print(f\"âœ… File already exists, skipping data fetching.\")\n",
    "    print(f\"   --> Using local file: {raw_data_path}\")\n",
    "    data_fetch_success = True\n",
    "else:\n",
    "    print(\"   --> Starting data fetching (local file not found or --force-rerun)...\")\n",
    "    returned_path = get_papers.main(config=config, raw_data_dir=data_raw_dir)\n",
    "    if not returned_path:\n",
    "        print(\"âŒ Data fetching failed. Terminating process.\")\n",
    "        data_fetch_success = False\n",
    "    else:\n",
    "        data_fetch_success = True\n",
    "        print(\"--- [Step 1/3] Data Fetching Complete ---\\n\")\n",
    "\n",
    "if not data_fetch_success:\n",
    "    # Stop execution if this step failed\n",
    "    raise SystemExit(\"Stopping notebook execution due to data fetching failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤äº”ï¼šä¸»é¢˜å»ºæ¨¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.get('topic_modeling', {}).get('enabled', False):\n",
    "    print(\"--- [Step 2/3] Topic Modeling ---\")\n",
    "    if not args.force_rerun and processed_data_path.exists():\n",
    "        print(f\"âœ… File already exists, skipping topic modeling.\")\n",
    "        print(f\"   --> Using local file: {processed_data_path}\")\n",
    "    else:\n",
    "        print(\"   --> Starting topic modeling (local file not found or --force-rerun)...\")\n",
    "        if not raw_data_path.exists():\n",
    "             print(f\"âŒ Input file {raw_data_path} does not exist. Cannot perform topic modeling.\")\n",
    "        else:\n",
    "            returned_path = run_topic_modeling.main(\n",
    "                config=config, input_path=raw_data_path, \n",
    "                processed_data_dir=data_processed_dir, output_dir=output_dir\n",
    "            )\n",
    "            if not returned_path:\n",
    "                print(\"âŒ Topic modeling failed. Terminating process.\")\n",
    "                raise SystemExit(\"Stopping notebook execution due to topic modeling failure.\")\n",
    "            else:\n",
    "                print(\"--- [Step 2/3] Topic Modeling Complete ---\\n\")\n",
    "else:\n",
    "    print(\"--- [Step 2/3] ğŸŸ¢ Topic modeling is disabled in the configuration. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤å…­ï¼šåˆ†æä¸å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.get('analysis', {}).get('enabled', False):\n",
    "    print(\"--- [Step 3/3] Analysis and Visualization ---\")\n",
    "    if processed_data_path.exists():\n",
    "        analyze.main(config=config, input_path=processed_data_path, output_dir=output_dir)\n",
    "    else:\n",
    "        print(f\"âš ï¸ Cannot perform analysis because the input file {processed_data_path} does not exist.\")\n",
    "    print(\"--- [Step 3/3] Analysis and Visualization Complete ---\\n\")\n",
    "else:\n",
    "    print(\"--- [Step 3/3] ğŸŸ¢ Analysis and visualization is disabled in the configuration. ---\")\n",
    "\n",
    "print(\"ğŸ‰ğŸ‰ğŸ‰ All processes completed successfully! ğŸ‰ğŸ‰ğŸ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_trend_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
